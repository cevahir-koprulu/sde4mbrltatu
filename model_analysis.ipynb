{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "from typing import Any, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from nsdes_dynamics.load_learned_nsdes import (\n",
    "    load_system_sampler_from_model_name,\n",
    ")\n",
    "\n",
    "from nsdes_dynamics.utils_for_d4rl_mujoco import (\n",
    "    load_dataset_for_nsdes,\n",
    "    get_environment_infos_from_name,\n",
    ")\n",
    "\n",
    "from nsdes_dynamics.dataset_op import (\n",
    "    pick_batch_transitions_from_trajectory_as_array\n",
    ")\n",
    "\n",
    "from nsdes_dynamics.parameter_op import (\n",
    "    pretty_print_config\n",
    ")\n",
    "\n",
    "# For plotting\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Magic IPython deepreloading...\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import builtins\n",
    "from IPython.lib import deepreload\n",
    "builtins.reload = deepreload.reload\n",
    "\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(env_name, verbose=False):\n",
    "    \"\"\" Load the dataset and print infos about the different environments\n",
    "    \"\"\"\n",
    "    dataset = load_dataset_for_nsdes(env_name)\n",
    "    if not verbose:\n",
    "        return dataset\n",
    "\n",
    "    print(f\"\\tNumber of trajectories: {len(dataset['trajectories'])}\")\n",
    "    trajectory_infos = dataset[\"trajectories_info\"]\n",
    "    print(\"\\tTrajectory information:\")\n",
    "    for info in trajectory_infos:\n",
    "        for _k in info.keys():\n",
    "            print(f\"\\t\\t{_k}: {info[_k]}\")\n",
    "    # Maximum values per field\n",
    "    max_value_per_field = dataset[\"max_values_per_field\"]\n",
    "    min_value_per_field = dataset[\"min_values_per_field\"]\n",
    "    mean_value_per_field = dataset[\"mean_values_per_field\"]\n",
    "    median_value_per_field = dataset[\"median_values_per_field\"]\n",
    "    print(\"\\tMaximum values per field:\")\n",
    "    for k in max_value_per_field.keys():\n",
    "        print(f\"\\t\\t{k}: {max_value_per_field[k]}\")\n",
    "    print(\"\\tMinimum values per field:\")\n",
    "    for k in min_value_per_field.keys():\n",
    "        print(f\"\\t\\t{k}: {min_value_per_field[k]}\") \n",
    "    print(\"\\tMean values per field:\")\n",
    "    for k in mean_value_per_field.keys():\n",
    "        print(f\"\\t\\t{k}: {mean_value_per_field[k]}\")\n",
    "    print(\"\\tMedian values per field:\")\n",
    "    for k in median_value_per_field.keys():\n",
    "        print(f\"\\t\\t{k}: {median_value_per_field[k]}\")\n",
    "    # Print the fields in the dataset\n",
    "    print(f\"\\tFields in the dataset: {dataset['data_fields']}\")\n",
    "    return dataset\n",
    "\n",
    "def plot_dataset(\n",
    "    dataset : Dict[str, Any],\n",
    "    indx_traj : int,\n",
    "    xaxis_name : str = \"time\",\n",
    "    per_subplot_figsize : Tuple[float, float] = (3.5, 3.5)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset\n",
    "            dict\n",
    "        indx_traj: The index of the trajectory to plot\n",
    "            int\n",
    "    \"\"\"\n",
    "    print(f\"Plotting the trajectory {indx_traj} of the dataset\")\n",
    "    print(dataset[\"trajectories_info\"][indx_traj])\n",
    "    trajectories = dataset[\"trajectories\"]\n",
    "    num_traj = len(trajectories)\n",
    "    if indx_traj >= num_traj:\n",
    "        raise ValueError(\n",
    "            f\"The index of the trajectory should be less than {num_traj}\"\n",
    "        )\n",
    "\n",
    "    # Field to plot\n",
    "    field_to_plot = [field for field in dataset[\"data_fields\"] if field != xaxis_name]\n",
    "    x_array = trajectories[indx_traj][xaxis_name]\n",
    "\n",
    "    # Create the figure with the subplots\n",
    "    num_cols = 4\n",
    "    num_rows = len(field_to_plot) // num_cols\n",
    "    if len(field_to_plot) % num_cols != 0:\n",
    "        num_rows += 1\n",
    "    single_subplot_fig_size = per_subplot_figsize\n",
    "    _, axs = plt.subplots(\n",
    "        num_rows, num_cols, figsize=(num_cols * single_subplot_fig_size[0],\n",
    "        num_rows * single_subplot_fig_size[1]),\n",
    "        sharex=True\n",
    "    )\n",
    "\n",
    "    # Plot the data\n",
    "    for i, field in enumerate(field_to_plot):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        axs[row, col].plot(x_array, trajectories[indx_traj][field])\n",
    "        axs[row, col].set_ylabel(field)\n",
    "        if row == num_rows - 1:\n",
    "            axs[row, col].set_xlabel(xaxis_name)\n",
    "        axs[row, col].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do model prediction and error computation\n",
    "def model_xpred_from_trajs(\n",
    "    sampling_fn,\n",
    "    trajectory: Dict[str, np.ndarray],\n",
    "    trajectory_info: Dict[str, Any],\n",
    "    rng_key: jnp.ndarray,\n",
    "    names_states: List[str],\n",
    "    names_controls: List[str],\n",
    "    horizon_pred: int,\n",
    "    num_splits: int = 5,\n",
    "    num_steps: int = 1\n",
    ") -> Dict[str, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict trajectories using the given model and the trajectory.\n",
    "    \n",
    "    Args:\n",
    "        sampling_fn: The sampling function\n",
    "            Callable\n",
    "        trajectory: The trajectory\n",
    "            Dict[str, np.ndarray]\n",
    "        trajectory_info: The trajectory information\n",
    "            Dict[str, Any]\n",
    "        rng_key: The random key\n",
    "            jnp.ndarray\n",
    "        names_states: The names of the states\n",
    "            List[str]\n",
    "        names_controls: The names of the controls\n",
    "            List[str]\n",
    "    \n",
    "    Returns:\n",
    "        dict_xevol: The predicted trajectories.\n",
    "            Dict[str, jnp.ndarray]\n",
    "    \"\"\"\n",
    "    # Problem config to extract relevant info from dataset\n",
    "    problem_config = {\n",
    "        \"names_states\" : names_states + [\"time\"],\n",
    "        \"names_controls\" : names_controls,\n",
    "        \"time_dependent_parameters\" : []\n",
    "    }\n",
    "\n",
    "    # Some parameters needed for extracting sequences from the trajectory\n",
    "    data_num_points = len(trajectory[\"time\"])\n",
    "    num_points_predicton = num_steps * horizon_pred\n",
    "    num_sequences = (data_num_points - 1) // num_points_predicton\n",
    "    valid_num_traj_points = num_sequences * num_points_predicton\n",
    "\n",
    "    # Some checks\n",
    "    assert valid_num_traj_points > 0, \\\n",
    "        f\"valid_num_traj_points {valid_num_traj_points} should be greater than 0\"\n",
    "    assert num_splits > 0, \\\n",
    "        f\"num_splits {num_splits} should be greater than 0\"\n",
    "    assert num_splits <= num_sequences, \\\n",
    "        f\"num_splits {num_splits} should be less than \" + \\\n",
    "        f\"num_sequences {num_sequences}\"\n",
    "\n",
    "    # print(f\"valid_num_traj_points: {valid_num_traj_points}\")\n",
    "    # print(f\"num_splits: {num_splits}\")\n",
    "    # print(f\"num_sequences: {num_sequences}\")\n",
    "    # print(f\"num_points_predicton: {num_points_predicton}\")\n",
    "\n",
    "    # Iterate through the number of split trajectory and perform the prediction\n",
    "    num_sequences_split = num_sequences // num_splits\n",
    "\n",
    "    result_list = []\n",
    "    for n_split in range(num_splits):\n",
    "        # How many sequences to use for this iteration based on the split\n",
    "        num_splits_for_iter = \\\n",
    "            num_sequences_split if n_split < num_splits - 1 else \\\n",
    "            (num_sequences - n_split * num_sequences_split)\n",
    "        # num_splits_for_iter = num_sequences_split\n",
    "\n",
    "        # print(\"num_splits_for_iter: \", num_splits_for_iter)\n",
    "        states_list = []\n",
    "        controls_list = []\n",
    "\n",
    "        for i in range(num_splits_for_iter):\n",
    "            # Where to start the sequence\n",
    "            start_idx = n_split * num_sequences_split * num_points_predicton + \\\n",
    "                i * num_points_predicton\n",
    "\n",
    "            # The stepsizes to use for the sequence\n",
    "            stepsizes = np.array(\n",
    "                [ num_steps * j for j in range(horizon_pred+1)]\n",
    "            )\n",
    "            # print(\"start_idx: \", start_idx)\n",
    "            # print(\"stepsizes: \", stepsizes)\n",
    "\n",
    "            # Extract the sequence\n",
    "            states, controls, _ = \\\n",
    "                pick_batch_transitions_from_trajectory_as_array(\n",
    "                    trajectory, trajectory_info, start_idx, stepsizes,\n",
    "                    problem_config, {\"default\": \"first\"}\n",
    "                )\n",
    "\n",
    "            # Append the states, controls and time_dependent_parameters\n",
    "            states_list.append(states)\n",
    "            controls_list.append(controls)\n",
    "\n",
    "        # Merge the states, controls, and time_dependent_parameters\n",
    "        states = np.stack(states_list, axis=0)\n",
    "        controls = np.stack(controls_list, axis=0)\n",
    "        # print(\"states and controls: \", states.shape, controls.shape)\n",
    "\n",
    "        # Sample the trajectories\n",
    "        x_pred_list = []\n",
    "        for _x, _u in zip(states, controls):\n",
    "            rng_key, temp_key = jax.random.split(rng_key)\n",
    "            x_pred, _ = sampling_fn(\n",
    "                state=_x[0,:-1], control=_u, rng_key=temp_key\n",
    "            )\n",
    "            x_pred_list.append(x_pred)\n",
    "        x_pred = np.stack(x_pred_list, axis=0)\n",
    "\n",
    "        # Separate the time and the states from ground truth\n",
    "        time_gt = states[...,-1]\n",
    "        states_gt = states[...,:-1]\n",
    "        states_dictionary = {}\n",
    "        for _k, name in enumerate(names_states):\n",
    "            states_dictionary[name+\"_gt\"] = states_gt[..., _k]\n",
    "            states_dictionary[name+\"_pred\"] = x_pred[..., _k]\n",
    "        states_dictionary[\"time_pred\"] = time_gt\n",
    "        result_list.append(states_dictionary) # Append the results\n",
    "\n",
    "    # Stack the results along the batch dimension\n",
    "    stacked_results = {\n",
    "        key: np.concatenate([res[key] for res in result_list], axis=0)\n",
    "        for key in result_list[0].keys()\n",
    "    }\n",
    "    return stacked_results\n",
    "\n",
    "\n",
    "def compute_error_metrics(\n",
    "    infos: Dict[str, np.ndarray],\n",
    "    names_states: List[str],\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute the error metrics for the predicted trajectories.\n",
    "    \n",
    "    Args:\n",
    "        infos: The information about the predicted trajectories.\n",
    "            Dict[str, np.ndarray]\n",
    "        names_states: The names of the states.\n",
    "            List[str]\n",
    "    \n",
    "    Returns:\n",
    "        error_metrics: The error metrics for the predicted trajectories.\n",
    "            Dict[str, np.ndarray]\n",
    "    \"\"\"\n",
    "    # Compute the error metrics\n",
    "    res_dict = {}\n",
    "    for name in names_states:\n",
    "        state_gt = infos[name + \"_gt\"] # [B, H]\n",
    "        state_pred = infos[name + \"_pred\"] # [B, Particle, H]\n",
    "\n",
    "        # Two type of errors. One is the error on the mean trajectory and\n",
    "        # the other is the mean of error over particle trajectories\n",
    "        mean_pred_state = np.mean(state_pred, axis=1)\n",
    "        error_on_mean = np.cumsum(\n",
    "            np.abs(mean_pred_state - state_gt), axis=-1\n",
    "        )\n",
    "\n",
    "        mean_on_error_val = np.abs(state_pred - \\\n",
    "            state_gt.reshape((state_gt.shape[0], 1, state_gt.shape[1])))\n",
    "        mean_on_error_std = np.cumsum(\n",
    "            np.std(mean_on_error_val, axis=1), axis=-1\n",
    "        )\n",
    "        mean_on_error_val = np.cumsum(\n",
    "            np.mean(mean_on_error_val, axis=1), axis=-1\n",
    "        )\n",
    "        res_dict[name + \"_mean_of_error\"] = mean_on_error_val\n",
    "        res_dict[name + \"_std_of_error\"] = mean_on_error_std\n",
    "        res_dict[name + \"_error_of_mean\"] = error_on_mean\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from models.tf_dynamics_models.bnn import BNN\n",
    "from models.tf_dynamics_models.constructor import construct_model\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "def load_ensemble(load_dir, task='halfcheetah-random-v2', algo='tatu_mopo'):\n",
    "    model_dir = os.path.join('log', task, algo, load_dir, 'dynamics_model')\n",
    "    env = gym.make(task)\n",
    "    obs_shape = env.observation_space.shape\n",
    "    action_dim = np.prod(env.action_space.shape)\n",
    "    dynamics_model = construct_model(\n",
    "        obs_dim=np.prod(obs_shape),\n",
    "        act_dim=action_dim,\n",
    "        hidden_dim=200,\n",
    "        num_networks=7,\n",
    "        num_elites=5,\n",
    "        model_type=\"mlp\",\n",
    "        separate_mean_var=True,\n",
    "        load_dir=model_dir,\n",
    "        name=\"BNN_0\",\n",
    "    )\n",
    "    return dynamics_model\n",
    "\n",
    "def predict_with_ensemble(dynamics_model, num_traj=1, deterministic=False):\n",
    "    def ensemble_sampling_fn(state, control, rng_key):\n",
    "        x  = state\n",
    "        us = control\n",
    "        x = np.ones((num_traj,x.shape[0]))*x \n",
    "        xs = [x]\n",
    "        stds = [np.zeros_like(x)]        \n",
    "        for u in us:\n",
    "            u_ = np.ones((num_traj, u.shape[0])) * u\n",
    "            inputs = np.concatenate((x, u_), axis=-1)\n",
    "            ens_model_means, ens_model_vars = dynamics_model.predict(inputs, factored=True)\n",
    "            ens_model_means = ens_model_means[:,:,1:] + x # Remove reward and move\n",
    "            ens_model_stds = np.sqrt(ens_model_vars[:,:,1:])\n",
    "            if deterministic:\n",
    "                ens_samples = ens_model_means\n",
    "                samples = np.mean(ens_samples, axis=0)\n",
    "                model_means = np.mean(ens_model_means, axis=0)\n",
    "                model_stds = np.mean(ens_model_stds, axis=0)\n",
    "            else:\n",
    "                ens_samples = ens_model_means + np.random.normal(size=ens_model_means.shape) * ens_model_stds\n",
    "                #### choose one model from ensemble\n",
    "                num_models = ens_model_means.shape[0]\n",
    "                model_inds = np.random.choice(num_models, size=num_traj)\n",
    "                samples = np.array([ens_samples[model_ind,i,:] for i, model_ind in enumerate(model_inds)])\n",
    "                model_means = np.array([ens_model_means[model_ind,i,:] for i, model_ind in enumerate(model_inds)])\n",
    "                model_stds = np.array([ens_model_stds[model_ind,i,:] for i, model_ind in enumerate(model_inds)])\n",
    "            x = samples\n",
    "            xs.append(x)\n",
    "            stds.append(model_stds)\n",
    "        return np.array(xs).transpose(1,0,2), np.array(stds).transpose(1,0,2)\n",
    "    return ensemble_sampling_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "dataset_name = \"halfcheetah-medium-expert-v2\"\n",
    "dataset = load_dataset(dataset_name, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indx_traj = 0\n",
    "# plot_dataset(dataset, indx_traj, \n",
    "#              xaxis_name =  \"time\",\n",
    "#              per_subplot_figsize= (3,3),\n",
    "# )\n",
    "\n",
    "# ensemble_dynamics = load_ensemble(load_dir=ensemble_model_name, task=dataset, algo='tatu_mopo')\n",
    "#     ensemble_sampling_fn = predict_with_ensemble(ensemble_dynamics, num_traj=num_sample, deterministic=deterministic_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "models_to_evaluate = \\\n",
    "[\n",
    "    # { # Base Fiala model\n",
    "    #     \"model_name\" : \"test__\",\n",
    "    #     \"plot_name\" : \"v1\",\n",
    "    # },\n",
    "    { # A learned model\n",
    "        \"model_name\" : \"test__\",\n",
    "        \"plot_name\" : \"Learned v1\",\n",
    "        \"step\" : -2, # The best model\n",
    "    },\n",
    "    {\n",
    "        \"model_name\" : \"critic_num_2_seed_32_0128_215130-halfcheetah_medium_expert_v2_tatu_mopo\",\n",
    "        \"plot_name\" : \"Ens\",\n",
    "        \"is_gaussian\" : True,\n",
    "    }\n",
    "]\n",
    "env_infos = get_environment_infos_from_name(dataset_name)\n",
    "data_stepsize = env_infos[\"stepsize\"]\n",
    "data_horizon = 20\n",
    "num_steps = 1 # 1 means stepsize is 0.05, 2 means stepsize is 0.1 etc.\n",
    "num_particles = 10\n",
    "names_states = env_infos[\"names_states\"]\n",
    "names_controls = env_infos[\"names_controls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "# Load the different models\n",
    "for i, model in enumerate(models_to_evaluate):\n",
    "    print(f\"{i+1}. Loading model {model['model_name']}\\n\")\n",
    "    model_name = model[\"model_name\"]\n",
    "    model_plot_name = model[\"plot_name\"]\n",
    "    model_step = model.get(\"step\", -2)\n",
    "    is_gaussian = model.get(\"is_gaussian\", False)\n",
    "    if not is_gaussian:\n",
    "        sampling_fn = load_system_sampler_from_model_name(\n",
    "            env_name = dataset_name, model_name = model_name,\n",
    "            stepsize = data_stepsize*num_steps, horizon = data_horizon,\n",
    "            step = model_step, integration_method=\"euler_maruyama\",\n",
    "            num_particles=num_particles\n",
    "        )\n",
    "        jit_sampling = jax.jit(sampling_fn)\n",
    "    else:\n",
    "        gaussian_model = load_ensemble(\n",
    "            model_name, task=dataset_name, algo='tatu_mopo'\n",
    "        )\n",
    "        jit_sampling = predict_with_ensemble(\n",
    "            dynamics_model=gaussian_model, num_traj=num_particles,\n",
    "            deterministic=False\n",
    "        )\n",
    "    models_dict[model_plot_name] = jit_sampling\n",
    "    print(f\"Model {model_name} loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute model predictions and model errors\n",
    "traj_indexes_to_evaluate = [0,1,2,3]\n",
    "pred_trajectory_res = []\n",
    "num_splits = 5 # For vmapping accross the computation -> the lower the more parallelism\n",
    "rng_key = jax.random.PRNGKey(10)\n",
    "for traj_id in traj_indexes_to_evaluate:\n",
    "    curr_trajectory = dataset[\"trajectories\"][traj_id]\n",
    "    curr_trajectory_info = dataset[\"trajectories_info\"][traj_id]\n",
    "    pretty_print_config(curr_trajectory_info)\n",
    "    temp_dict = {}\n",
    "    rng_key, model_key = jax.random.split(rng_key)\n",
    "    for model_name, model_sample in models_dict.items():\n",
    "        print(f\"Computing model predictions for model {model_name}...\")\n",
    "        pred_trajectory = model_xpred_from_trajs(\n",
    "            model_sample,\n",
    "            curr_trajectory,\n",
    "            curr_trajectory_info,\n",
    "            rng_key=model_key,\n",
    "            names_states=names_states,\n",
    "            names_controls=names_controls,\n",
    "            horizon_pred=data_horizon,\n",
    "            num_steps=num_steps,\n",
    "            num_splits=num_splits,\n",
    "        )\n",
    "        # Compute error metrics\n",
    "        error_metrics = compute_error_metrics(\n",
    "            pred_trajectory, names_states\n",
    "        )\n",
    "        temp_dict[model_name] = {\"pred\" : pred_trajectory,\n",
    "                                \"error_metrics\" : error_metrics\n",
    "                                }\n",
    "        print(f\"Done\\n\")\n",
    "    pred_trajectory_res.append((temp_dict, traj_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of colors for the plots\n",
    "colors = [\"b\", \"r\", \"g\", \"c\", \"m\", \"y\", \"k\"]\n",
    "color_per_model = {model_name : colors[i] for i, model_name in enumerate(models_dict.keys())}\n",
    "# Let's display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the full trajectory prediction of the states\n",
    "fields_to_plot = names_states\n",
    "xaxis_toplot = \"time\"\n",
    "# Lets plot some of the forces\n",
    "per_subplot_figsize = (3,3)\n",
    "max_num_cols = 4\n",
    "num_rows = len(fields_to_plot) // max_num_cols\n",
    "num_rows = num_rows if len(fields_to_plot) % max_num_cols == 0 else num_rows + 1\n",
    "num_cols = len(fields_to_plot) \\\n",
    "    if len(fields_to_plot) < max_num_cols else max_num_cols\n",
    "for (_pred_traj, _traj_id) in pred_trajectory_res:\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, \n",
    "                            figsize=(num_cols*per_subplot_figsize[0],\n",
    "                                        num_rows*per_subplot_figsize[1]),\n",
    "                            constrained_layout=True,\n",
    "                            sharex=True\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "    for i, field_name in enumerate(fields_to_plot):\n",
    "        ax = axs[i]\n",
    "        # Plot the ground truth first\n",
    "        ax.plot(dataset[\"trajectories\"][_traj_id][\"time\"], \n",
    "                dataset[\"trajectories\"][_traj_id][field_name], \n",
    "                label=\"Ground truth\" if i == 0 else None,\n",
    "                c=\"k\", linestyle=\"--\",\n",
    "                zorder=1000\n",
    "        )\n",
    "        # # Just plot the controls for the ground truth\n",
    "        # if field_name in [\"handwheel_angle\",\n",
    "        #         \"rear_roadwheel_motor_torque\",\n",
    "        #         \"front_roadwheel_brake_torque\", \n",
    "        #         \"rear_roadwheel_brake_torque\"\n",
    "        #         ]:\n",
    "        #     ax.set_xlabel(\"time\")\n",
    "        #     ax.set_ylabel(field_name)\n",
    "        #     ax.grid(True)\n",
    "        #     continue\n",
    "        for model_name, pred in _pred_traj.items():\n",
    "            prediction_model = pred[\"pred\"]\n",
    "            color = color_per_model[model_name]\n",
    "            xaxis_name = xaxis_toplot + \"_pred\"\n",
    "            yaxis_name = field_name + \"_pred\"\n",
    "            # Iteration over the different splits\n",
    "            first_val = i == 0\n",
    "            for _xval, _yval in zip(prediction_model[xaxis_name], \n",
    "                                    prediction_model[yaxis_name]):\n",
    "                # Iteration over the different samples\n",
    "                for _yval_sample in _yval:\n",
    "                    ax.plot(_xval, _yval_sample, \n",
    "                            label=model_name if first_val else None,\n",
    "                            c=color, linestyle=\"-\"\n",
    "                    )\n",
    "                    first_val = False\n",
    "        ax.set_xlabel(xaxis_toplot)\n",
    "        ax.set_ylabel(field_name)\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
